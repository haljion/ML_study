# 1. 準備
* 本書籍における定義もあるので一般化しすぎないほうがいいかも

## 入力データについて
* 教師あり学習における訓練データ
  * 入力訓練データX: m行n列の行列
  * 出力訓練データy: ザイズ(要素数)がmのベクトル
* `サンプル`
  * 入力データのi行目と出力訓練データのi個目を取り出したもの
  * 調査対象の個体に対応している(ワインとか)
  * サンプルごとの特徴量の次元: `d`
* xi: 入力訓練データXのi行目(サンプルi)を縦ベクトルにしたもの

### 重み
![](img/05/機械学習1.PNG)
* 線形和で行列データが与えられることもある
* 重みに掛けられるベクトルvに~(チルダ)が付いているかどうかで定数項ありのデータか否かを判別する

![](img/05/機械学習2.PNG)

### 用語
* `特徴量行列`
  * 入力訓練データX の一般名
  * 教師あり、なしどちらでも使う
* `特徴量ベクトル`
  * Xの中のサンプル xi
* 教師ありデータではX,yが訓練データ、教師なしはXのみ
* `回帰`
  * 教師あり学習の中でも、特に出力訓練データの値の大小に意味がある場合(ワインの品質とか)
    * 回帰の時の出力訓練データを`ターゲット`という
* `分類`
  * 出力訓練データの値の大小に意味がない場合(あやめの花の分類とか)
  * 出力訓練データを`ラベル`という

### インターフェース
* 教師あり学習

```python
# 機械学習アルゴリズムを実装したクラス インスタンス化
model = Algorithm(parameters)
# X: 特徴量行列、 y: ターゲット(ラベル) 学習
model.fit(X, y)
# X_test: 評価用データ 評価用データについての予測値を取得
y_predicted = model.predict(X_test)
```

* クラスタリング(教師なし学習)
  * 点群のかたまりを見つけ出すタスク
  * 学習結果は、各点がどのクラスタに属するかという識別データ(ラベル)になる

```python
# インスタンス化
model = Algorithm(parameters)
# 学習(教師なしなのでyはなし)
model.fit(X)
# ラベルの取得
clusters = model.labels_
```

* 次元圧縮(教師なし学習)
  * 多次元空間上の点群をて異次元空間に射影する
  * 点群からどの方向に射影すべきかを学習し、その後に新たに与えられた点群を射影する

```python
# インスタンス化
model = Algorithm(parameters)
# 学習(教師なしなのでyはなし)
model.fit(X)
# 射影
Z = model.transfer(X)
```

# 2. 回帰
## 特徴量ベクトルが1次元の場合
### 原点を通る直線による近似
* `最小二乗法`
  * 直線(y=ax)で近似した時の誤差の2乗の和を最小化する
  * 近似としてよく使われる手法
![](img/05/機械学習3.PNG)

### 一般の直線を通る直線による近似
* 一般の直線: y=ax+b
![](img/05/機械学習4.PNG)
![](img/05/機械学習5.PNG)
![](img/05/機械学習6.PNG)

### 特徴量ベクトルが多次元の場合
* `線形回帰`
  * 式
  * ![](img/05/機械学習7.PNG)
    * ![](img/05/機械学習8.PNG)
    * 同じ
  * (x0,...,xd)^T: 入力変数
  * (w0,...,wd)^T: パラメータ
    * 1次元の場合、w0: b、w1: a
  * y: ターゲット
  * ε: ノイズ
* wの計算式
  * ![](img/05/機械学習9.PNG)

* 線形回帰の実装
  * linearreg.py
```python
import numpy as np
from scipy import linalg

class LinearRegression:
  def __init__(self):
    self.w = None

  def fit(self, X, t):
    # shape[0]で行数を取得し、n行1列の1行列を生成
    # c_でXと横連結し、~Xに相当する行列を生成
    Xtil = np.c_[np.ones(X.shape[0]), X]
    # ~X^T ~X
    A = np.dot(Xtil.T, Xtil)
    # ~X^T y
    b = np.dot(Xtil.T, t)
    # 1次関数を解く関数 bはn行1列なので1次関数とみなせる？
    self.w_ = linalg.solve(A, b)

  def predict(self, X):
    # 配列の次元(Pythonにおける)数
    # ベクトルの時？
    if X.ndim == 1:
      # 1行にする(=1行ベクトルにする？)
      X = X.reshape(1, -1)
    Xtil = np.c_[np.ones(X.shape[0]), X]
    # ~X^T w
    return np.dot(Xtil, self.w_)
```

* 乱数を使った人工データによる上記モデルの実験
  * 訓練入力データの次元数は2, 100個のサンプル(`n`)を生成
  * 線形和`w0 + w1x1 + w2x2`について、w0=1, w1=2, w2=3としている
* `np.random.randn()`
  * 正規分布に従う乱数を要素に持つ行列を生成する
```python
# 上記で作成したモデル
import linearreg
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d

n = 100
scale = 10

np.random.seed(0)
# 要素が乱数のサイズ100*2の行列を生成 0から10までの値
X = np.random.random((n, 2)) * scale
w0 = 1
w1 = 2
w2 = 3
# 線形和 + 乱数
y = w0 + w1 * X[:, 0] + w2  * X[:, 1] + np.random.randn(n)

# 作成したモデルのインスタンス
model = linearreg.LinearRegression()
model.fit(X, y)
print("係数：", model.w_)
print("(1, 1)に対する予測値：", model.predict(np.array([1, 1])))

xmesh, ymesh = np.meshgrid(
  np.linspace(0, scale, 20),
  np.linspace(0, scale, 20)
  )

zmesh = (
  model.w_[0]
  + model.w_[1] * xmesh.ravel()
  + model.w_[2] * ymesh.ravel()
 ) * reshape(xmesh.shape)

fig = flt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X[:, 0], X[:, 1], y, color="k")
ax.plot_wireframe(xmesh, ymesh, zmesh, color="r")
plt.show()
```
![](img/05/機械学習10.PNG)

* `RMSE(Root of Mean Square Error)`: 平均二乗誤差のルート
  * 予測の評価によく使われる
  * このような指標値は他モデルとの優劣を測るために使用する どれくらいだと良いのかは場合による
![](img/05/機械学習11.PNG)

* `ホールド・アウト検証`
  * データを訓練用と評価用に分け、訓練データで学習し、評価用データをうまく予測できているかを評価する手法
  * 機械学習アルゴリズムの性能評価として共通で用いられるもの
  * 既知のデータのみから未知のデータを予測する、機械学習システムの目標としている状況を擬似的に作り出している

# 3. リッジ回帰
* `リッジ回帰`
  * 線形回帰で最小化する目的関数にパラメータの大きさ(L2ノルム)の項を足したもの
  * ![](img/05/機械学習12.PNG)
  * `λ||w||^2`: `正規化項`
  * 正規化項が加わったことにより、点群を線形に維持するだけでなく、できるだけwのL2ノルムが小さい方がよくなる
  * λ: `ハイパーパラメータ`
    * wの大きさをどのくらい重視するかを表す定数

* ![](img/05/機械学習13.PNG)
  * 上が普通の線形回帰、下がリッジ回帰
  * 上の方が点が増えるに付き直線の傾きが大きく変化している
  * リッジ回帰では比較的はずれ値から受ける影響が少ない
    * サンプル数が少ないときでも例外的なデータからの影響が少ない(正規化項の効果)
    * いい事かどうかは場合による

## ハイパーパラメータとチューニング
* `ハイパーパラメータ`
  * モデルが学習を始める前にあらかじめ値を決め、ずっとそのまま変化しない値
  * 他の変化するパラメータがどのように変化していくのかを決定するもの
  * ひたすらいろんな値を試してチューニングするしかない

# 4. 汎化と過学習
## 多項式回帰における例
* `多項式回帰`
  * 入力関数xに対して出力yがxの多項式関数で表されるモデル
  * xが1次元の場合の例(dは次数, εはノイズ)
  * ![](img/05/機械学習14.PNG)
  * x^i を xi に置き換えると線形回帰の公式になる
    * 計算的には、要素が以下になった線形回帰である
    * ![](img/05/機械学習15.PNG)
* `np.concatenate()`
  * ベクトルを横に繋いだ行列を作る
* ![](img/05/機械学習16.PNG)
  * 実践が多項式回帰、破線が線形回帰
  * 上記例においては、多項式の方が与えられた点によく当てはまっているが、性能はあまりよくない
  * **訓練データを完璧に正しく予想できるよう学習したモデルが良いとは限らない**
* `汎化性能`
  * 未知のデータをどのくらい予測できるかという性能のこと
  * 機械学習システムを利用する目的の1つ

## モデルの汎化性能
* 2乗誤差の平均
* ![](img/05/機械学習17.PNG)
  * `第1項: バイアス`
    * すべてのデータについての予測値の平均と真の値の2乗
    * 真の値が変わらない前提の場合、xにおける予測値の平均と真の値の差がどのくらいあるかを意味する
    * 観測データ(実際のデータ？)が変わった時に、平均的にどのくらい予想できるかという値
  * `第2項: バリアンス`
    * データが変化した時の予測値の分散
      * 式違うかも？分散を計算する
    * 観測データが変わった時に、平均的に予測値がばらつくかという値
  * 予測誤差の平均 = バイアス + バリアンス

### 例題
![](img/05/機械学習18.PNG)
![](img/05/機械学習19.PNG)
* バイアスだけ見ると線形回帰より多項式回帰のほうが良いモデルに見えるが...

![](img/05/機械学習20.PNG)
* 左: 線形回帰、右: 多項式回帰
  * 多項式回帰ではx>3の範囲でバリアンス(予測と真の値の差)がばらつく
  * バイアス: 小 かつ バリアンス: 大 → 平均は真に近いが、大外しも多い
* `ax.fill_between(x, y1, y2)`
  * ２つの関数のy軸方向のx区間を塗りつぶす

## 交差検証
* (ハイパーパラメータの)`過学習`
  * テストデータについては上手く予測できるが、一般のデータについてはあまりよく予測できないような値をハイパーパラメータの値として選択してしまうこと

* `交差検証(クロスバリデーション)`
  * k分割交差検証(kは数字)
    * データをk個に分割して、そのk個のデータのうち1つをテスト用にして他を訓練用にする
    * ![](img/05/機械学習21.PNG)
    * 各回の評価が終わるとモデルは捨てられ、ゼロから学習し直し、複数回の評価の平均を全体の評価値とする

# 5. ラッソ回帰
* 正規化項として`L1ノルム`を加えたものを`ラッソ回帰`と呼ぶ
  * リッジ回帰ではL2ノルムが正規化項だった
  * 以下を最小化するwを求める
  * ![](img/05/機械学習22.PNG)

## 座標降下法(coordinate descent)
* ![](img/05/機械学習23.PNG)
* ![](img/05/機械学習24.PNG)
* ![](img/05/機械学習26.PNG)
  * x0w0 = w0であることに注目
* ![](img/05/機械学習25.PNG)
  * S(p,q)のpに当たる部分が正 = wk+
  * S(p,q)のpに当たる部分が負 = wk-
  * ![](img/05/機械学習27.PNG)

### ソフト閾値関数
![](img/05/機械学習28.PNG)
* wkの更新において、wkの値は0からスタートし、一定の条件下では上記グラフの通りwkは0のままである
* 上記から、ラッソ回帰の解はほとんどの要素が0になる(疎になる)
* `スパーシティ`: 全体に対するゼロ要素の割合
  * λを小さくするとスパーシティが下がる(非ゼロが増える)
    * S(X,λ)のグラフや式より、xの絶対値がλ以下だとyは0になる
    * よって、λが大きいほど0になりやすく、スパーシティが上がる

# 6. ロジスティック回帰
* 主に二値分類(ラベルの値が2種類しかないような教師あり訓練データ)に使われるアルゴリズム
* ラベルの値が0または1の場合
  * 与えられた特徴量のサンプルx(∈R^d)に対して、ラベルyが1になる確率
  * P(Y = 1|X = x)
* ロジスティック回帰は以下の式を前提にしたモデルである
  * ![](img/05/機械学習29.PNG)
  * σ: `シグモイド関数`

# 7. サポートベクタマシン
# 8. k-Means法
# 9. 主成分分析(PCA)
